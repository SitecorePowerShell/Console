----item----
version: 1
id: {5A752D27-E9A0-4273-A5A8-270B3CE91E52}
database: master
path: /sitecore/system/Modules/PowerShell/Script Library/Platform/Functions/Get-DTWFileEncoding
parent: {E22D066A-04D0-4799-9DAD-EDD9EB07C2B2}
name: Get-DTWFileEncoding
master: {00000000-0000-0000-0000-000000000000}
template: {DD22F1B3-BD87-4DB2-9E7D-F7A496888D43}
templatekey: PowerShell Script

----field----
field: {B1A94FF0-6897-47C0-9C51-AA6ACB80B1F0}
name: Script
key: script
content-length: 9024

#region Function: Get-DTWFileEncoding

<#
.SYNOPSIS
Returns the encoding type of the file
.DESCRIPTION
Returns the encoding type of the file.  It first attempts to determine the 
encoding by detecting the Byte Order Marker using Lee Holmes' algorithm
(http://poshcode.org/2153).  However, if the file does not have a BOM
it makes an attempt to determine the encoding by analyzing the file content
(does it 'appear' to be UNICODE, does it have characters outside the ASCII
range, etc.).  If it can't tell based on the content analyzed, then 
it assumes it's ASCII. I haven't checked all editors but PowerShell ISE and 
PowerGUI both create their default files as non-ASCII with a BOM (they use
Unicode Big Endian and UTF-8, respectively).  If your file doesn't have a 
BOM and 'doesn't appear to be Unicode' (based on my algorithm*) but contains 
non-ASCII characters after index ByteCountToCheck, the file will be incorrectly
identified as ASCII.  So put a BOM in there, would ya!

For more information and sample encoding files see:
http://danspowershellstuff.blogspot.com/2012/02/get-file-encoding-even-if-no-byte-order.html
And please give me any tips you have about improving the detection algorithm.

*For a full description of the algorithm used to analyze non-BOM files, 
see "Determine if Unicode/UTF8 with no BOM algorithm description".
.PARAMETER Path
Path to file
.PARAMETER ByteCountToCheck
Number of bytes to check, by default check first 10000 character.
Depending on the size of your file, this might be the entire content of your file.
.PARAMETER PercentageMatchUnicode
If pecentage of null 0 value characters found is greater than or equal to
PercentageMatchUnicode then this file is identified as Unicode.  Default value .5 (50%)
.EXAMPLE
Get-IHIFileEncoding -Path .\SomeFile.ps1 1000
Attempts to determine encoding using only first 1000 characters
BodyName          : unicodeFFFE
EncodingName      : Unicode (Big-Endian)
HeaderName        : unicodeFFFE
WebName           : unicodeFFFE
WindowsCodePage   : 1200
IsBrowserDisplay  : False
IsBrowserSave     : False
IsMailNewsDisplay : False
IsMailNewsSave    : False
IsSingleByte      : False
EncoderFallback   : System.Text.EncoderReplacementFallback
DecoderFallback   : System.Text.DecoderReplacementFallback
IsReadOnly        : True
CodePage          : 1201
#>
function Get-DTWFileEncoding {
  #region Function parameters
  [CmdletBinding()]
  param(
    [Parameter(Mandatory = $true,ValueFromPipeline = $true,ValueFromPipelineByPropertyName = $true)]
    [ValidateNotNullOrEmpty()]
    [Alias("FullName")]
    [string]$Path,
    [Parameter(Mandatory = $false)]
    [int]$ByteCountToCheck = 10000,
    [Parameter(Mandatory = $false)]
    [decimal]$PercentageMatchUnicode = .5
  )
  #endregion
  process {
    # minimum number of characters to check if no BOM
    [int]$MinCharactersToCheck = 400
    #region Parameter validation
    #region SourcePath must exist; if not, exit
    if ($false -eq (Test-Path -Path $Path)) {
      Write-Error -Message "$($MyInvocation.MyCommand.Name) :: Path does not exist: $Path"
      return
    }
    #endregion
    #region ByteCountToCheck should be at least MinCharactersToCheck
    if ($ByteCountToCheck -lt $MinCharactersToCheck) {
      Write-Error -Message "$($MyInvocation.MyCommand.Name) :: ByteCountToCheck should be at least $MinCharactersToCheck : $ByteCountToCheck"
      return
    }
    #endregion
    #endregion

    #region Determine file encoding based on BOM - if exists
    # the code in this section is mostly Lee Holmes' algorithm: http://poshcode.org/2153
    # until we determine the file encoding, assume it is unknown
    $Unknown = "UNKNOWN"
    $result = $Unknown

    # The hashtable used to store our mapping of encoding bytes to their
    # name. For example, "255-254 = Unicode"
    $encodings = @{}

    # Find all of the encodings understood by the .NET Framework. For each,
    # determine the bytes at the start of the file (the preamble) that the .NET
    # Framework uses to identify that encoding.
    $encodingMembers = [System.Text.Encoding] | Get-Member -Static -MemberType Property
    $encodingMembers | ForEach-Object {
      $encodingBytes = [System.Text.Encoding]::($_.Name).GetPreamble() -join '-'
      $encodings[$encodingBytes] = $_.Name
    }

    # Find out the lengths of all of the preambles.
    $encodingLengths = $encodings.Keys | Where-Object { $_ } | ForEach-Object { ($_ -split "-").Count }

    # Go through each of the possible preamble lengths, read that many
    # bytes from the file, and then see if it matches one of the encodings
    # we know about.
    foreach ($encodingLength in $encodingLengths | Sort-Object -Descending) {
      $bytes = (Get-Content -Path $Path -Encoding byte -ReadCount $encodingLength)[0]
      $encoding = $encodings[$bytes -join '-']

      # If we found an encoding that had the same preamble bytes,
      # save that output and break.
      if ($encoding) {
        $result = $encoding
        break
      }
    }
    # if encoding determined from BOM, then return it
    if ($result -ne $Unknown) {
      [System.Text.Encoding]::$result
      return
    }
    #endregion

    #region No BOM on file, attempt to determine based on file content
    #region Determine if Unicode/UTF8 with no BOM algorithm description
    <#
       Looking at the content of many code files, most of it is code or
       spaces.  Sure, there are comments/descriptions and there are variable
       names (which could be double-byte characters) or strings but most of
       the content is code - represented as single-byte characters.  If the
       file is Unicode but the content is mostly code, the single byte
       characters will have a null/value 0 byte as either as the first or
       second byte in each group, depending on Endian type.
       My algorithm uses the existence of these 0s:
        - look at the first ByteCountToCheck bytes of the file
        - if any character is greater than 127, note it (if any are found, the 
          file is at least UTF8)
        - count the number of 0s found (in every other character)
          - if a certain percentage (compared to total # of characters) are 
            null/value 0, then assume it is Unicode
          - if the percentage of 0s is less than we identify as a Unicode
            file (less than PercentageMatchUnicode) BUT a character greater
            than 127 was found, assume it is UTF8.
          - Else assume it's ASCII.
       Yes, technically speaking, the BOM is really only for identifying the
       byte order of the file but c'mon already... if your file isn't ASCII
       and you don't want it's encoding to be confused just put the BOM in
       there for pete's sake.
       Note: if you have a huge amount of text at the beginning of your file which
       is not code and is not single-byte, this algorithm may fail.  Again, put a 
       BOM in.
    #>
    #endregion
    $Content = (Get-Content -Path $Path -Encoding byte -ReadCount $ByteCountToCheck -TotalCount $ByteCountToCheck)
    # get actual count of bytes (in case less than $ByteCountToCheck)
    $ByteCount = $Content.Count
    [bool]$NonAsciiFound = $false
    # yes, the big/little endian sections could be combined in one loop
    # sorry, crazy busy right now...

    #region Check if Big Endian
    # check if big endian Unicode first - even-numbered index bytes will be 0)
    $ZeroCount = 0
    for ($i = 0; $i -lt $ByteCount; $i += 2) {
      if ($Content[$i] -eq 0) { $ZeroCount++ }
      if ($Content[$i] -gt 127) { $NonAsciiFound = $true }
    }
    if (($ZeroCount / ($ByteCount / 2)) -ge $PercentageMatchUnicode) {
      # create big-endian Unicode with no BOM
      New-Object System.Text.UnicodeEncoding $true,$false
      return
    }
    #endregion

    #region Check if Little Endian
    # check if little endian Unicode next - odd-numbered index bytes will be 0)
    $ZeroCount = 0
    for ($i = 1; $i -lt $ByteCount; $i += 2) {
      if ($Content[$i] -eq 0) { $ZeroCount++ }
      if ($Content[$i] -gt 127) { $NonAsciiFound = $true }
    }
    if (($ZeroCount / ($ByteCount / 2)) -ge $PercentageMatchUnicode) {
      # create little-endian Unicode with no BOM
      New-Object System.Text.UnicodeEncoding $false,$false
      return
    }
    #endregion

    #region Doesn't appear to be Unicode; either UTF8 or ASCII
    # Ok, at this point, it's not a Unicode based on our percentage rules
    # if not Unicode but non-ASCII character found, call it UTF8 (no BOM, alas)
    if ($NonAsciiFound -eq $true) {
      New-Object System.Text.UTF8Encoding $false
      return
    } else {
      # if made it this far, I'm calling it ASCII; done deal pal
      [System.Text.Encoding]::"ASCII"
      return
    }
    #endregion
    #endregion
  }
}
#endregion

----version----
language: en
version: 1
revision: 56a5618d-8699-40fe-a507-48c63787a120

----field----
field: {25BED78C-4957-4165-998A-CA1B52F67497}
name: __Created
key: __created
content-length: 15

20150625T105727
----field----
field: {8CDC337E-A112-42FB-BBB4-4143751E123F}
name: __Revision
key: __revision
content-length: 36

56a5618d-8699-40fe-a507-48c63787a120
----field----
field: {D9CF14B1-FA16-4BA6-9288-E8A174D4D522}
name: __Updated
key: __updated
content-length: 34

20150625T105727:635708266477753906
----field----
field: {BADD9CF9-53E0-4D0C-BCC0-2D784C282F6A}
name: __Updated by
key: __updated by
content-length: 14

sitecore\admin
